package ai.koog.workshop.intro

import ai.koog.prompt.dsl.prompt
import ai.koog.prompt.executor.clients.openai.OpenAIModels
import ai.koog.prompt.executor.llms.all.simpleOpenAIExecutor
import kotlinx.coroutines.runBlocking

// TODO:
//  1. Create a prompt executor
//  2. Come up with system and user message and execute the prompt
//  3. Make executor stream the output using [PromptExecutor.executeStreaming]
fun main() {
    // Create a prompt with a system and a user message
    val prompt = prompt("my-prompt") {
        system("Your system message")
        user("Your user message")
    }
    prompt.messages.forEach { println(it) }

    // Most of the providers require an api token to be used
    val token = System.getenv("OPENAI_API_KEY") ?: error("OPENAI_API_KEY is required.")

    // Executor is the wrapper around the llm provider, it sends the prompt to the llm and returns the result
    val executor = simpleOpenAIExecutor(token)

    // The llm execution is asynchronous, so we need to run it in a coroutine
    runBlocking {
        val result = executor.execute(
            prompt = prompt,
            model = OpenAIModels.Chat.GPT4_1,
            tools = emptyList()
        )

        // The result is a message generated by the llm as a response to the prompt
        println(result)
    }
}