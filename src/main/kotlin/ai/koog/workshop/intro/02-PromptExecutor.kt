package ai.koog.workshop.intro

import ai.jetbrains.code.prompt.llm.JetBrainsAIModels
import ai.koog.prompt.dsl.prompt
import ai.koog.workshop.intro.utils.simpleGraziePromptExecutor
import kotlinx.coroutines.runBlocking

// TODO:
//  1. Create a prompt executor
//  2. Come up with system and user message and execute the prompt
//  3. Make executor stream the output using [PromptExecutor.executeStreaming]
fun main() {
    // Create a prompt with a system and a user message
    val prompt = prompt("my-prompt") {
        system("Your system message")
        user("Your user message")
    }
    prompt.messages.forEach { println(it) }

    // Most of the providers require an api token to be used
    val token = System.getenv("GRAZIE_TOKEN") ?: error("GRAZIE_TOKEN is required.")

    // Executor is the wrapper around the llm provider, it sends the prompt to the llm and returns the result
    val executor = simpleGraziePromptExecutor(token)

    // The llm execution is asynchronous, so we need to run it in a coroutine
    runBlocking {
        val result = executor.execute(
            prompt = prompt,
            model = JetBrainsAIModels.OpenAI_GPT4_1_via_JBAI,
            tools = emptyList()
        )

        // The result is a message generated by the llm as a response to the prompt
        println(result)
    }
}